{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7123955-d3f0-41f7-a4c7-a1c7e3fd7971",
   "metadata": {},
   "source": [
    "SemForms automatically mines code from large GitHub repositories of existing code that may be operating on the same dataset to construct features for AutoML.  It uses static analysis of existing Python code to extract features used in that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a8c0b-1d81-48ba-bbd0-0c75dc8354b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_example import handle_transforms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "import numpy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0870d87-813e-45c7-8d97-ceab782c3304",
   "metadata": {},
   "source": [
    "Consider the housing dataset from OpenML as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bbd41-3c12-483b-a3b1-ec65042479bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "dataset      = fetch_openml('houses', version=1) # name is dataset_name\n",
    "X            = dataset['data']\n",
    "target       = dataset['target'].to_frame()\n",
    "\n",
    "print(X)\n",
    "print(target)\n",
    "print(dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3a7f5-8f26-4fac-a8fb-06e2c8dba598",
   "metadata": {},
   "source": [
    "We use a random forest regressor on this dataset to find a baseline R^2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93814d-6974-4904-acec-29367c4dc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set standard classifier (could be any AutoML as well)\n",
    "estimator = RandomForestRegressor(random_state = 1908)\n",
    "# Evaluate on original data\n",
    "scores = cross_val_score(estimator, X, numpy.ravel(target), cv=3, scoring='r2')\n",
    "print(\"Averaged r2 score on original data:  \" + str(statistics.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c264c3-bfac-4a8e-b6a0-d3bc03ad0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86712ce3-142b-47de-837d-1dfa1b913af7",
   "metadata": {},
   "source": [
    "To find relevant scripts, we can use the GitHub API to find Python notebooks that have mentions of the table name and columns.  For this demo, to avoid issues with networking, GitHub access tokens and search non-determinism, we use a set of Python files that we have found before with a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cbb24-614b-405d-9cf4-a8030d8b98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://raw.githubusercontent.com/sayemimtiaz/kaggle-notebooks/4b639bfedf2245b288d0b0d02dcbfa838a18caf0/notebooks/ritabanmitra/californiahousingreg.ipynb\",\n",
    "\"https://raw.githubusercontent.com/sayemimtiaz/kaggle-notebooks/4b639bfedf2245b288d0b0d02dcbfa838a18caf0/notebooks/obrunet/california-housing-prices.ipynb\",\n",
    "\"https://raw.githubusercontent.com/pollozhao/JupyterNotebook/424e39b10a35b9bcbb1833c14d5776b68460c8a4/pysparkML.ipynb\",\n",
    "\"https://raw.githubusercontent.com/sayemimtiaz/kaggle-notebooks/4b639bfedf2245b288d0b0d02dcbfa838a18caf0/notebooks/ronishternberg/roni-california-housing-prices.ipynb\",\n",
    "\"https://raw.githubusercontent.com/sayemimtiaz/kaggle-notebooks/4b639bfedf2245b288d0b0d02dcbfa838a18caf0/notebooks/ashokshamnani/handson-book-end-to-end-ml.ipynb\",\n",
    "\"https://raw.githubusercontent.com/sayemimtiaz/kaggle-notebooks/4b639bfedf2245b288d0b0d02dcbfa838a18caf0/notebooks/harrywang/housing-price-prediction.ipynb\",\n",
    "\"https://raw.githubusercontent.com/francisco-renteria/francisco-renteria.github.io/59427cce8ca3bd71b9968f65c52332a5ba884df5/HOML/OEBPS/ch02.html\",\n",
    "\"https://raw.githubusercontent.com/binarybrain-009/M.L.-learning-and-reading-material/4d119e1d49bbfe8427645ae4090114bc5b1acf1f/01_end_to_end_machine_learning_project-checkpoint.ipynb\",\n",
    "\"https://raw.githubusercontent.com/sudhanshu1402/Basic-Machine-Learning-Projects/a07146832e2289f1b9b9924a0806f74e63c375b6/Housing%20Prices%20Prediction/Housing%20Prices%20Prediction.ipynb\",\n",
    "\"https://raw.githubusercontent.com/CS196Illinois/Group33-FA22/52fc391f2344df3c7c1d62ada34fe09e30dd5a56/Project/Backend/price_prediction_models.ipynb\",\n",
    "\"https://raw.githubusercontent.com/Sayar1106/Sayar1106.github.io/45899ac1f8e6368a1f066cde46e9968f6ffa68f0/post/index.xml\",\n",
    "\"https://raw.githubusercontent.com/arunpeddakotla/hansonml2/72501d47132a18da11e741db27c30c3061c1b9f0/2.%20End-to-End%20Machine%20Learning%20Project%20-%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn,%20Keras,%20and%20TensorFlow,%202nd%20Edition.html\",\n",
    "\"https://raw.githubusercontent.com/Sayar1106/portfolio-website/484cfe9ba95f69085dc11cd0d543ca4799d040b6/public/post/index.xml\",\n",
    "\"https://raw.githubusercontent.com/Sayar1106/portfolio-website/484cfe9ba95f69085dc11cd0d543ca4799d040b6/public/authors/admin/index.xml\",\n",
    "\"https://raw.githubusercontent.com/Sayar1106/Sayar1106.github.io/45899ac1f8e6368a1f066cde46e9968f6ffa68f0/index.xml\",\n",
    "\"https://raw.githubusercontent.com/DAI-Lab/RP_Master_thesis/b76f726c0bb2917cde8d476124383ebebca354aa/N1_extract_data_generate_synt_data.ipynb\",\n",
    "\"https://raw.githubusercontent.com/dmsenter89/dmsenter89.github.io/e6664e7c90bf902accaf848fedbe5a30acc906ea/post/index.xml\",\n",
    "\"https://raw.githubusercontent.com/dmsenter89/dmsenter89.github.io/e6664e7c90bf902accaf848fedbe5a30acc906ea/index.xml\",\n",
    "\"https://raw.githubusercontent.com/stigbosmans/MLEX/55de97bb7f94293bba8f0b29107ebd48e84ab278/CH2/.ipynb_checkpoints/HousingPrices-checkpoint.ipynb\",\n",
    "\"https://raw.githubusercontent.com/stigbosmans/MLEX/55de97bb7f94293bba8f0b29107ebd48e84ab278/CH2/HousingPrices.ipynb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f165793-c496-4ede-b95c-a570e32417c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import logging\n",
    "import nbformat\n",
    "from nbconvert.exporters import PythonExporter\n",
    "\n",
    "def get_python(nbhandle):\n",
    "    # read source notebook\n",
    "    print('reading:' + nbhandle)\n",
    "    f = urllib.request.urlopen(nbhandle)\n",
    "    try:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "        python_exporter = PythonExporter()\n",
    "        code = python_exporter.from_notebook_node(nb)\n",
    "        return code[0]\n",
    "    except:\n",
    "        # logging.exception(\"message\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea0801-5b8b-4834-bbad-8e96d2262312",
   "metadata": {},
   "source": [
    "Once have the relevant URLs on GitHub, we can access the source code, and submit it to a program analysis script.  This script analyzes creation of pandas dataframe objects, and any reads or writes to column names in the dataset, and any sort of operations performed on them.  The extracted code is then normalized into a set of Python lambda expressions so it can be dropped into an AutoML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229298eb-ed44-484c-b90b-4dab86eb1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "expressions = []\n",
    "code2count = {}\n",
    "code2url = {}\n",
    "\n",
    "for url in urls:\n",
    "    code = get_python(url)\n",
    "    if code is None:\n",
    "        continue\n",
    "    req = {'repo': url, 'source': code, 'indexName': 'expressions'}\n",
    "    response = requests.post('http://localhost:4567/index', json=req)\n",
    "    for r in response.json():\n",
    "        if r['code'] not in code2count:\n",
    "            code2count[r['code']] = 0\n",
    "            code2url[r['code']] = r['source_file']\n",
    "        code2count[r['code']] += 1\n",
    "\n",
    "codes = {k: v for k, v in sorted(code2count.items(), reverse=True, key=lambda item: item[1])[:10]}\n",
    "\n",
    "for idx, c in enumerate(codes):\n",
    "    expressions.append({'expr_name': 'expr' + str(idx), 'code': c, 'url': code2url[c]})\n",
    "\n",
    "print(expressions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8289e-559e-428f-84d0-c486cdb85f1a",
   "metadata": {},
   "source": [
    "As you can see from the example, not all expressions that are extracted can be applied. Either the column names might not exist (e.g., income_cat) or the expressions might yield a single value and so cannot be used as a feature, or it may be directly correlated with an existing column.  This next step filters out what cannot be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29238d3-fd6e-407e-8a7c-8926cc055e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze given transforms and if applicable create SKLEARN Function Transforms as a pipeline\n",
    "transforms_suggested, correlation, generated_code = handle_transforms(\"both\", expressions, target, X, 'houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941b0e1-06ef-4489-9d3f-423b370d4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what augmentions are kept\n",
    "generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565fd88-20e8-486b-8e17-54d64be08edc",
   "metadata": {},
   "source": [
    "Now we just prepend this set of suggested transformers to our existing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4e0a7-132e-428f-a0d4-66c70172523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator to suggested transformation pipeline\n",
    "transforms_suggested.append(('estimator', estimator))\n",
    "pipeline = Pipeline(transforms_suggested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f392b-a3c5-4b73-8e5f-53d0ba0af411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with data augmentation added as function transformers based on original data\n",
    "scores = cross_val_score(pipeline, X, numpy.ravel(target), cv=3, scoring='r2')\n",
    "print(\"Averaged r2 score with augmentations on original data:  \" + str(statistics.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90131ef5-ea50-4e62-a78e-448df4c792b0",
   "metadata": {},
   "source": [
    "As we can see, in the case of this dataset, we see an improvement from adding mined features.  Not every dataset will show this improvement, but this is in the spirit of AutoML.  AutoML explores many features and pipelines for datasets, and is not guaranteed to improve performance for every dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
